<div align="center">
<h1>
  BlueLM
</h1>
</div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/vivo-ai" target="_blank">Hugging Face</a> â€¢ ğŸ‘¾ <a href="https://www.modelscope.cn/organization/vivo-ai" target="_blank">ModelScope</a> â€¢ ğŸ¤– <a href="https://www.wisemodel.cn/organization/vivo-ai" target="_blank">wisemodel</a> â€¢ ğŸ“œ <a href="MODEL_LICENSE_EN.pdf" target="_blank">LICENSE</a> â€¢ ğŸ¯ <a href="https://developers.vivo.com/product/ai/bluelm" target="_blank">vivo Developers</a> â€¢ ğŸ—¨ <a href="resources/wechat.png" target="_blank">WeChat</a>
</p>

<div align="center">

<h4 align="center">
    <p>
        <b>English</b> |
        <a href="README.md">ä¸­æ–‡</a>
    <p>
</h4>

</div>

# Table of Contents

- [ğŸ“” Models Introduction](#Models-Introduction)
- [ğŸ“Š Benchmark Results](#Benchmark-Results)
- [ğŸš€ Inference and Deployment](#Inference-and-Deployment)
- [âš’ Fine-tuning the Model](#Fine-tuning-the-Model)
- [ğŸ“š Disclaimer, License and Citation](#Disclaimer-License-and-Citation)
- [ğŸ“  Contact Us](#Contact-Us)

# Models Introduction

BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

- **High-quality Data**: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
- **Stronger Performance**: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
- **Longer Context**: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
- **Model License**: BlueLM weights are open for academic research and commercial use. 

The release versions and hugging face download links are listed in the table below:

|        | Base Model                                                                      | Chat Model                                                                     | 4bits Quantized Chat Model                                                                          |
|:-------|:--------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|
| 7B-2K  | ğŸ¤— [BlueLM-7B-Base](https://huggingface.co/vivo-ai/BlueLM-7B-Base)              | ğŸ¤— [BlueLM-7B-Chat](https://huggingface.co/vivo-ai/BlueLM-7B-Chat)             | ğŸ¤— [BlueLM-7B-Chat-4bits](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-4bits)                      |
| 7B-32K | ğŸ¤— [BlueLM-7B-Base-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Base-32K)      | ğŸ¤— [BlueLM-7B-Chat-32K](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-32K)     | -                                                                                                   |

Welcome to read our technical report [BlueLM: An Open Multilingual 7B Language Model](https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf)!

We will release the 13B language model and the 7B-vl multi-modal language model soon!

# Benchmark Results

To ensure the consistency of model evaluation, we use [OpenCompass](https://opencompass.org.cn/leaderboard-llm) to evaluate the performance on relevant leaderboards. We conducted extensive tests on C-Eval, MMLU, CMMLU, GaoKao, AGIEval, BBH, GSM8K, MATH and HumanEval datasets across general ability, mathematical ability and coding ability.

## Benchmarks

- [C-Eval](https://cevalbenchmark.com/index.html) is the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context.
- [MMLU](https://arxiv.org/abs/2009.03300) covers 57 tasks including elementary mathematics, US history, computer science, law, and more.
- [CMMLU](https://github.com/haonan-li/CMMLU) is a comprehensive Chinese evaluation benchmark specifically designed to assess the knowledge and reasoning abilities of language models in the context of the Chinese language.
- [Gaokao](https://github.com/OpenLMLab/GAOKAO-Bench) is an evaluation framework that utilizes Chinese high school entrance examination (GAOKAO) questions as a dataset to evaluate the language understanding and logical reasoning abilities of large language models.
- [AGIEval](https://github.com/ruixiangcui/AGIEval) is a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving.
- [BBH](https://huggingface.co/datasets/lukaemon/bbh) is a suite of 23 challenging BIG-Bench tasks.
- [GSM8K](https://github.com/openai/grade-school-math) is a dataset of 8.5K high quality linguistically diverse grade school math word problems created by human problem writers.
- [MATH](https://huggingface.co/datasets/competition_math) is a new dataset of 12,500 challenging competition mathematics problem.
- [HumanEval](https://huggingface.co/datasets/openai_humaneval) is to measure functional correctness for synthesizing programs from docstrings.
- [LongBench](https://github.com/THUDM/LongBench) is the first benchmark for bilingual, multitask, and comprehensive assessment of long context understanding capabilities of large language models.

## 7B Model Results

| Model             | **C-Eval** | **MMLU** | **CMMLU**  | **Gaokao**   | **AGIEval**   | **BBH**      | **GSM8K**   | **MATH**   | **HumanEval**   |
|:------------------|:-----------|:---------|:-----------|:-------------|:--------------|:-------------|:------------|:-----------|:----------------|
|                   | 5-shot     | 5-shot   | 5-shot     | 0-shot       | 0-shot        | 3-shot       | 4-shot      | 5-shot     | 0-shot          |
| GPT-4             | 69.9       | **86.4** | 71.2       | **72.3**     | **55.1**      | **86.7**     | **91.4**    | **45.8**   | **74.4**        |
| ChatGPT           | 52.5       | 70.0     | 53.9       | 51.1         | 39.9          | 70.1         | 78.2        | 28         | 73.2            |
| LLaMA2-7B         | 32.5       | 45.3     | 31.8       | 18.9         | 21.8          | 38.2         | 16.7        | 3.3        | 12.8            |
| ChatGLM2-6B(Base) | 51.7       | 47.9     | 50.0       | -            | -             | 33.7         | 32.4        | 6.5        | -               |
| Baichuan2-7B      | 56.3       | 54.7     | 57.0       | 34.8         | 34.6          | 41.8         | 24.6        | 5.4        | 17.7            |
| BlueLM-7B-Base    | 67.5       | **55.2** | 66.6       | **58.9**     | **43.4**      | 41.7         | 27.2        | 6.2        | 18.3            |
| BlueLM-7B-Chat    | **72.7**   | 50.7     | **74.2**   | 48.7         | **43.4**      | **65.6**     | **51.9**    | **13.4**   | **21.3**        |

## 7B-32K Model Results

We also tested our BlueLM-7B-Chat-32K  on the LongBench dataset and the results are shown in the table below:

| Model                  | Average  | Summary  | Single-Doc QA | Multi-Doc QA  | Code  | Few-shot | Synthetic |
|:-----------------------|:---------|:---------|:--------------|:--------------|:------|:---------|:----------|
| BlueLM-7B-Chat-32K     | 41.2     | 18.8     | 35.6          | 36.2          | 54.2  | 56.9     | 45.5      |

# Inference and Deployment

## Dependency Installation

You need to download this repository to use BlueLM:

```
git clone https://github.com/vivo-ai-lab/BlueLM
cd BlueLM
```

Install dependencies with pip:

```
pip install -r requirements.txt
```

When using BlueLM-7B-Base-32K or BlueLM-7B-Chat-32K, please install flash_attn additionally:

```
pip install flash_attn==2.3.3
```

If the installation fails, it is recommended to install [pre-build wheel file](https://github.com/Dao-AILab/flash-attention/releases/) of flash_attn.

## Usage

### Base Model

```python
>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> tokenizer = AutoTokenizer.from_pretrained("vivo-ai/BlueLM-7B-Base", trust_remote_code=True, use_fast=False)
>>> model = AutoModelForCausalLM.from_pretrained("vivo-ai/BlueLM-7B-Base", device_map="cuda:0", trust_remote_code=True)
>>> model = model.eval()
>>> inputs = tokenizer("å„’æ—å¤–å²->å´æ•¬æ¢“\néš‹å”æ¼”ä¹‰->è¤šäººè·\nçº¢æ¥¼æ¢¦->", return_tensors="pt")
>>> inputs = inputs.to("cuda:0")
>>> pred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)
>>> print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
å„’æ—å¤–å²->å´æ•¬æ¢“
éš‹å”æ¼”ä¹‰->è¤šäººè·
çº¢æ¥¼æ¢¦->æ›¹é›ªèŠ¹
ä¸‰å›½æ¼”ä¹‰->ç½—è´¯ä¸­
æ°´æµ’ä¼ ->æ–½è€åºµ
è¥¿æ¸¸è®°->å´æ‰¿æ©
èŠæ–‹å¿—å¼‚->è’²æ¾é¾„
å°ç¥æ¼”ä¹‰->è®¸ä»²ç³
ä¸œå‘¨åˆ—å›½å¿—->å†¯æ¢¦é¾™
ä¸‰ä¾ äº”ä¹‰->çŸ³ç‰æ˜†
ä¸ƒå‰‘åä¸‰ä¾ ->å”èŠ¸æ´²
```

## Command Line Demo

![cli-demo](resources/cli_demo.gif)

```shell
python cli_demo.py
```

This command line demo is only designed for Chat Model.

## Web Demo

![web-demo](resources/web_demo.gif)

```shell
streamlit run web_demo.py --server.port 8080
```

This web demo is only designed for Chat Model.

## Demo Results of BlueLM-7B-Chat

<details><summary><b>Code</b></summary>

`````
Userï¼šæŸåœ¨çº¿æ•™è‚²å¹³å°çš„è¯¾ç¨‹æ•°æ®å­˜å‚¨åœ¨ä¸€ä¸ªåä¸ºcoursesçš„åˆ—è¡¨ä¸­ï¼Œæ¯é—¨è¯¾ç¨‹ç”¨å­—å…¸è¡¨ç¤ºï¼ŒåŒ…å«è¯¾ç¨‹åç§°ï¼ˆnameï¼‰ã€æ•™å¸ˆå§“åï¼ˆteacherï¼‰å’Œå­¦ç”Ÿäººæ•°ï¼ˆstudentsï¼‰ä¸‰ä¸ªå­—æ®µã€‚è¯·ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œæ ¹æ®å­¦ç”Ÿäººæ•°ï¼Œè®¡ç®—è¯¾ç¨‹çš„å¹³å‡å­¦ç”Ÿäººæ•°ï¼Œå¹¶è¿”å›ã€‚

BlueLM-7Bï¼šä¸‹é¢æ˜¯å®ç°ä»£ç ï¼š

```python
def average_students(courses):
    total_students = 0
    count = 0
    for course in courses:
        total_students += course['students']
        count += 1
    if count == 0:
        return 0
    else:
        return total_students / count
```

è¿™ä¸ªå‡½æ•°æ¥å—ä¸€ä¸ªè¯¾ç¨‹åˆ—è¡¨ä½œä¸ºå‚æ•°ï¼Œç„¶åéå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªè¯¾ç¨‹ï¼Œç´¯åŠ å­¦ç”Ÿäººæ•°çš„æ€»å’Œï¼ŒåŒæ—¶è®°å½•è¯¾ç¨‹çš„æ•°é‡ã€‚æœ€åï¼Œå¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œåˆ™è¿”å›0ï¼›å¦åˆ™ï¼Œè¿”å›å­¦ç”Ÿäººæ•°çš„å¹³å‡å€¼ã€‚
`````

</details>

<details><summary><b>Math</b></summary>

`````
Userï¼šæœµæœµæœ‰100å…ƒï¼Œå¥¹è¦ä¹°ä¸¤æœ¬ç»˜ç”»ä¹¦ï¼Œç»˜ç”»ä¹¦æ¯æœ¬40å…ƒï¼Œæœµæœµä¹°å®Œç»˜ç”»ä¹¦è¿˜å‰©å¤šå°‘å…ƒï¼Ÿ

BlueLM-7Bï¼šå¯ä»¥å…ˆè®¡ç®—ä¸¤æœ¬ç»˜ç”»ä¹¦éœ€è¦æ”¯ä»˜çš„æ€»é‡‘é¢ï¼Œç„¶åè®¡ç®—è´­ä¹°åè¿˜å‰©ä¸‹çš„é‡‘é¢ã€‚

å·²çŸ¥æœµæœµæœ‰çš„é’±æ•°ä¸ºï¼š100å…ƒï¼›

å·²çŸ¥æ¯æœ¬ç»˜ç”»ä¹¦çš„ä»·æ ¼ä¸ºï¼š40å…ƒï¼›

å› æ­¤ï¼Œä¸¤æœ¬ç»˜ç”»ä¹¦éœ€è¦æ”¯ä»˜çš„æ€»é‡‘é¢ä¸ºï¼š$40 \times 2 = 80$å…ƒï¼›

è´­ä¹°ä¸¤æœ¬ç»˜ç”»ä¹¦åï¼Œè¿˜å‰©ä¸‹çš„é‡‘é¢ä¸ºï¼š$100 - 80 = 20$å…ƒï¼›

æ‰€ä»¥ï¼Œæœµæœµä¹°å®Œç»˜ç”»ä¹¦è¿˜å‰©ä¸‹20å…ƒã€‚
`````

</details>

<details><summary><b>Role Play</b></summary>

`````
Userï¼šæˆ‘æ˜¯ä¸€åè¥é”€ç»ç†ï¼Œæœ€è¿‘æˆ‘ä»¬æ¨å‡ºäº†ä¸€æ¬¾æ–°äº§å“ï¼Œä½†æ˜¯é”€å”®æƒ…å†µå¹¶ä¸ç†æƒ³ã€‚è¿™æ¬¾äº§å“æ˜¯ä¸€æ¬¾å¥åº·é£Ÿå“ï¼Œä¸»æ‰“ä½è„‚ä½ç³–ï¼Œé€‚åˆå¥èº«äººç¾¤å’Œéœ€è¦æ§åˆ¶é¥®é£Ÿçš„äººç¾¤ã€‚æˆ‘ä»¬å·²ç»å°è¯•è¿‡çº¿ä¸Šè¥é”€ã€çº¿ä¸‹æ¨å¹¿ã€ä¸çŸ¥åå¥èº«æˆ¿åˆä½œç­‰æ–¹å¼ï¼Œä½†æ˜¯æ•ˆæœéƒ½ä¸æ˜æ˜¾ã€‚ä½ èƒ½å¸®æˆ‘åˆ†æä¸€ä¸‹å¯èƒ½çš„åŸå› ä»¥åŠæä¾›ä¸€äº›é”€å”®ç­–ç•¥å—ï¼Ÿ

BlueLM-7Bï¼šåˆ†æï¼š

1. ç«äº‰æ¿€çƒˆï¼šå¸‚åœºä¸Šå·²ç»å­˜åœ¨è®¸å¤šç±»ä¼¼çš„äº§å“ï¼Œæ¶ˆè´¹è€…æœ‰æ›´å¤šçš„é€‰æ‹©ï¼Œå¯¼è‡´æ–°äº§å“çš„å¸‚åœºå æœ‰ç‡ä¸ºé›¶æˆ–å¾ˆä½ã€‚

2. ä»·æ ¼è¿‡é«˜ï¼šè¯¥äº§å“çš„ä»·æ ¼æ¯”åŒç±»å…¶ä»–äº§å“æ›´é«˜ï¼Œå¯èƒ½ä¼šä½¿æ¶ˆè´¹è€…æ›´å€¾å‘äºè´­ä¹°å…¶ä»–å“ç‰Œæˆ–æ›´ä¾¿å®œçš„é€‰é¡¹ã€‚

3. ç¼ºä¹å®£ä¼ ï¼šæ²¡æœ‰è¶³å¤Ÿçš„å®£ä¼ ä½¿å¾—æ¶ˆè´¹è€…å¯¹è¯¥äº§å“çš„äº†è§£ç¨‹åº¦ä¸å¤Ÿï¼Œä»è€Œæ— æ³•å¸å¼•æ›´å¤šçš„æ¶ˆè´¹è€…ã€‚

4. é”€å”®æ¸ é“ä¸å½“ï¼šè™½ç„¶å·²ç»å°è¯•äº†å¤šç§é”€å”®æ–¹å¼ï¼Œä½†å¯èƒ½æ²¡æœ‰é€‰æ‹©åˆ°æœ€é€‚åˆçš„æ¸ é“ï¼Œå¯¼è‡´é”€å”®æƒ…å†µä¸ä½³ã€‚

ç­–ç•¥ï¼š

1. é™ä½ä»·æ ¼ï¼šè€ƒè™‘é™ä½ä»·æ ¼ä»¥å¸å¼•æ›´å¤šçš„æ¶ˆè´¹è€…ï¼ŒåŒæ—¶ä¿æŒäº§å“è´¨é‡å’Œå“è´¨ã€‚

2. åŠ å¼ºå®£ä¼ ï¼šå¢åŠ å®£ä¼ åŠ›åº¦ï¼ŒåŒ…æ‹¬åœ¨ç¤¾äº¤åª’ä½“ä¸Šå‘å¸ƒå¹¿å‘Šã€å¢åŠ çº¿ä¸Šè¥é”€ç­‰æ–¹å¼ï¼Œæé«˜æ¶ˆè´¹è€…å¯¹äº§å“çš„è®¤çŸ¥åº¦ã€‚

3. å¯»æ‰¾åˆé€‚çš„é”€å”®æ¸ é“ï¼šè€ƒè™‘ä¸å¥èº«æˆ¿ç­‰æœºæ„è¿›è¡Œåˆä½œï¼Œæˆ–è€…å¯»æ‰¾æ›´ç¬¦åˆç›®æ ‡æ¶ˆè´¹è€…çš„é”€å”®æ¸ é“ã€‚

4. æ”¹è¿›äº§å“ï¼šæ ¹æ®æ¶ˆè´¹è€…çš„åé¦ˆæ„è§ï¼Œå¯¹äº§å“è¿›è¡Œæ”¹è¿›ï¼Œæé«˜å…¶è´¨é‡å’Œå—æ¬¢è¿ç¨‹åº¦ã€‚
`````
</details>

## Quantization Deployment

We release a pre-quantized 4bits version Chat Model: [BlueLM-7B-Chat-4bits](https://huggingface.co/vivo-ai/BlueLM-7B-Chat-4bits) for download. If your GPU memory is limited, you can try to load the model parameters with quantization. It only requires 5GB GPU memory. Moreover, BlueLM has been tested to smoothly generate text even under 4-bit quantization.

Install dependencies:

```
cd BlueLM/quant_cuda
python setup_cuda.py install
```

You can load and inference in the following manner:

```python
>>> from transformers import AutoTokenizer, AutoModelForCausalLM
>>> tokenizer = AutoTokenizer.from_pretrained("vivo-ai/BlueLM-7B-Chat-4bits", trust_remote_code=True, use_fast=False)
>>> model = AutoModelForCausalLM.from_pretrained("vivo-ai/BlueLM-7B-Chat-4bits", device_map="cuda:0", trust_remote_code=True)
>>> model = model.eval()
>>> inputs = tokenizer("[|Human|]:ä¸‰å›½æ¼”ä¹‰çš„ä½œè€…æ˜¯è°ï¼Ÿ[|AI|]:", return_tensors="pt")
>>> inputs = inputs.to("cuda:0")
>>> outputs = model.generate(**inputs, max_new_tokens=128)
>>> print(tokenizer.decode(outputs.cpu()[0], skip_special_tokens=True))
ä¸‰å›½æ¼”ä¹‰çš„ä½œè€…æ˜¯è°ï¼Ÿ ã€Šä¸‰å›½æ¼”ä¹‰ã€‹æ˜¯ç”±å…ƒæœ«æ˜åˆå°è¯´å®¶ç½—è´¯ä¸­æ‰€è‘—ï¼Œæ˜¯ä¸­å›½å¤å…¸å››å¤§åè‘—ä¹‹ä¸€ï¼Œä¹Ÿæ˜¯ä¸­å›½å¤ä»£å†å²å°è¯´å‘å±•çš„å·…å³°ä¹‹ä½œã€‚
```

## Inference Acceleration

### Install vLLM

We added BlueLM inference code based on the [vllm](https://github.com/vllm-project/vllm) inference framework. The code is in the `example/vllm` directory.

The required version of the NVIDIA driver to install is 525.125.06, and the CUDA version should be 12.1.

```
python -m venv vllm
source vllm/bin/activate

cd example/vllm
pip install -e .
``` 

### Inference Demo

```
python vllm_demo.py
```

# Fine-tuning the Model

## Dependency Installation

```
pip install deepspeed==0.10.3
```

## Training Data

To demonstrate the fine-tuning process of our model in a simplified way, we selectively extracted 10,000 Chinese instruction data from the [BELLE project 500k Chinese instruction dataset](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN) to serve as a demonstrative dataset. The data has been processed and can be accessed at `data/bella_train_demo.json` and `data/bella_dev_demo.json`.

## Fine-Tuning

After obtaining the processed data, you can perform fine-tuning training by configuring the corresponding paths and hyperparameters through the training script `script/bluelm-7b-sft.sh`.

The description of the relevant parameters is as follows:

| Parameter                       | Description                                                                                |
|:--------------------------------|:-------------------------------------------------------------------------------------------|
| **num_gpus**                    | Number of GPUs to use                                                                      |
| **train_file**                  | The path of train file                                                                     |
| **prompt_column**               | The name of prompt column                                                                  |
| **response_column**             | The name of response column                                                                |
| **model_name_or_path**          | Storage path for the preloaded model                                                       |
| **output_dir**                  | Storage path for the fine-tuned model                                                      |
| **tensorboard_dir**             | Storage path for the tensorboard                                                           |
| **seq_len**                     | Maximum length of training sequence                                                        |
| **batch_size_per_device**       | Number of samples per GPU input during training iteration                                  |
| **gradient_accumulation_steps** | Step length for gradient accumulation. Default is 1, which means no gradient accumulation  |
| **gradient_checkpointing**      | Whether to use gradient checkpointing                                                      |
| **max_steps**                   | Number of iterations for model training                                                    |
| **save_steps**                  | The interval step of model saving                                                          |
| **learning_rate**               | Learning rate                                                                              |
| **finetune**                    | Whether to enable fine-tuning                                                              |

You can start fine-tuning training using the following command:

```sh
cd train
sh script/bluelm-7b-sft.sh
```

## Fine-Tuning with LoRA

This project supports fine-tuning with LoRA. For detailed information about LoRA, please refer to the paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) and the Github repository [LoRA](https://github.com/microsoft/LoRA)ã€‚

The description of the relevant parameters is as follows:

| Parameter                       | Description                                                                              |
|:--------------------------------|:-----------------------------------------------------------------------------------------|
| **lora_rank**                   | The rank of the LoRA matrix. Generally set to 8, 16, 32, 64, etc                         |
| **lora_alpha**                  | LoRA alpha. Generally set to 16, 32, and so on.                                          |
| **lora_dropout**                | The dropout rate of the LoRA.                                                            |

You can start fine-tuning training with LoRA using the following command:

```sh
cd train
sh script/bluelm-7b-sft-lora.sh
```

# Disclaimer, License and Citation

## Disclaimer

We hereby declare strongly that all parties using the BlueLM models should not engage in any behavior that may damage national or social security, or violate relevant laws. We also request users not to use BlueLM model in product applications that have not been properly security-approved and registered. Please be sure to conduct all business activities under the premise of legality and compliance. We expect all users to comply with this.

This model is provided "as is". We have done our best to ensure the compliance of our traning data. Due to the complexity of the model and data, there may still be unforeseeable issues. We also strongly recommend users to conduct a detailed risk assessment of the model to ensure the legal compliance of the application. We will not assume any responsibility for any problems caused by using the BlueLM models.

## License

Our code is licensed under the [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) and [Community License for BlueLM Model](MODEL_LICENSE_EN.pdf). The BlueLM weights are completely open for academic research, and free commercial use is allowed after completing the [questionnaire](https://developers.vivo.com/buc/bluelm/apply).

## Citation

```
@misc{2023bluelm,
    title={BlueLM: An Open Multilingual 7B Language Model},
    author={BlueLM Team},
    howpublished = {\url{https://github.com/vivo-ai-lab/BlueLM}},
    year={2023}
}
```

# Contact Us

If you have any questions about the BlueLM, you can contact us via email (developers-ai@vivo.com). You can also scan the QR code to join the WeChat group.

![wechat](resources/wechat.png)
